{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.15rc1\r\n"
     ]
    }
   ],
   "source": [
    "!python -V\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networkx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6f9bf122e69f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mttest_ind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'networkx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import mannwhitneyu\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import Counter\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = pd.read_csv('metro.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Check for any columns with null fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only latitudes and longitudes may contain null values. Station IDs may be able to give us the latitude.\n",
    "From the data description, the station ID of 3000 is a virtual station used for maintenance etc. Below checks which station IDs are associated with null lat/long values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = list()\n",
    "bike_df[bike_df['start_lat'].isnull() | bike_df['start_lon'].isnull() | bike_df['end_lat'].isnull() | bike_df['end_lon'].isnull()][['start_station', 'start_lat', 'start_lon', 'end_station', 'end_lat', 'end_lon']].apply(lambda x: (s.append((x['start_station'], x['start_lat'], x['start_lon'])), s.append((x['end_station'], x['end_lat'], x['end_lon']))), axis=1);\n",
    "# generate a list of tuple (station id, lat, lon) where lat or lon are missing\n",
    "s = set(map(lambda x: (x[0], None if pd.isnull(x[1]) else x[1], None if pd.isnull(x[2]) else x[2]), s))\n",
    "# get unique list of tuples and convert pandas null fields to python's None - required for next step\n",
    "s_lat_long_NaN = list(filter(lambda x: True if (x[1] is None and x[2] is None) else False, s))\n",
    "# get only tuples where both lat and lon are null\n",
    "print(s_lat_long_NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, station 3000 (the virtual station) is the only station where lat and lon are null. Depending on the analysis being made it may be wise to remove these from the dataset as they are non-standard values (unless the goal is to try and classify these!). The distribution of duration associated with these trips is far higher than standard (statistical proof below in the 'Distribution of duration' section).\n",
    "\n",
    "The assumption made here is that the goal is not to try and classify or investigate the journeys involving station 3000, therefore any journeys involving this station are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned = bike_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block above drops all journeys containing null values, which we have shown to be only the journeys involving the virtual station 'station 3000'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretations of the above statistics must be made with caution, as statistics relating to fields other than duration are likely to be useless for interpretation, Eg. the median bike_id. Non-numeric columns are also not shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned['passholder_type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned['trip_route_category'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see some basic statistics for the numeric columns, and the normalized category counts for each 'passholder type' and 'trip route category'.\n",
    "\n",
    "We can see that the vast majority of journeys are conducted by people with 'Walk-up' or 'Monthly' passes, with ~6% of journeys being from 'One Day', 'Flex', and 'Annual' passes. This difference between the two may cause the distributions of other fields with regard to 'passholder type' to be harder to compare due to massively varying sample sizes.\n",
    "\n",
    "The majority of journeys are also 'One Way', once again making the distributions of other fields with regard to 'trip route category' harder to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned['duration'].plot.hist(bins=100, logy=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_duration_station_3000 = bike_df[bike_df['start_lat'].isnull() | bike_df['end_lat'].isnull()]\n",
    "# null values for start_lat/end_lat indicate station 3000 (virtual station)\n",
    "bike_df_duration_station_3000['duration'].plot.hist(bins=100, logy=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above it can be seen that the trips associated with bike station 3000 have a disproportionately high probability of having the maximum duration value, 1440 minutes, when compared to journeys not involving station 3000. This claim is validated below through statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bike_df_duration_station_3000['duration'].describe())\n",
    "print(\"\")\n",
    "print(bike_df_cleaned['duration'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the count and standard deviation of each is very different which may signify that these samples are from two different distibutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mannwhitneyu(bike_df_duration_station_3000['duration'], bike_df_cleaned['duration']))\n",
    "print(ks_2samp(bike_df_duration_station_3000['duration'], bike_df_cleaned['duration']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0 -> They are the same distribution\n",
    "\n",
    "H1 -> They follow different distributions\n",
    "\n",
    "**Mann-Whitney U-test Assumptions:**\n",
    "- One dependant variable (duration)\n",
    "- One independant variable that is split into two classes (with/without station 3000)\n",
    "- Observations for each class (sample) are independant (no journey is in both classes)\n",
    "- Distribution of the dependant variable for each class is of a similar shape (seen by histograms)\n",
    "\n",
    "**Two sample Kolmogorov-Smirnov test Assumptions:**\n",
    "- Observations for each class (sample) are independant (no journey is in both classes)\n",
    "- One ordinal dependant variable (duration)\n",
    "- Exact only for continuous variables, conservative for descrete variables\n",
    "\n",
    "With a pvalues of ~0.0 from Mann-Whitney U and Kolmogorov-Smirnov tests, H0 is rejected (and H1 accepted) at any confidence interval. Therefore the distribution of these durations is different, and the inclusion of the durations involving bike station 3000 would skew the duration value; providing a potentially inaccurate representation of journey durations.\n",
    "\n",
    "**Note**: It is assumed that the p-values are greater than 0, however are represented as 0.0 due to floating point or other rounding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned['duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew(bike_df_cleaned['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned['duration'].plot.hist(bins=100, logy=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duration field is skewed positively, as shown by the 'skew' statistic above, also shown by the fact that the median < mean. The majority of the data is contained within a small range of values, as shown by the interquartile range (25th to 75th percentiles). As can be seen from the histogram above, there seems to be a disproportionate amount of values which take the maximum values, however this can be explained with the datasheet and data description as 'Trip lengths are capped at 24 hours' which equates to 1440 minutes.\n",
    "\n",
    "The median shows that 50% of the journeys are under 14 minutes and the high standard deviation value signifies that the duration variable is well spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of passholder type and duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned[['passholder_type','duration']].groupby('passholder_type').filter(lambda x: True if x.plot.hist(logy=True, bins=100) is not None else False);\n",
    "# filter was used above as the 'apply' function for 'groupby' object duplicates the first group, and draws the first group's graph twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the corresponding distributions for the duration variable for passholder types Annual Pass, Flex Pass, Monthly Pass, One Day Pass, and Walk-up respectively.\n",
    "\n",
    "Note that the number of samples within these distributions differs.\n",
    "\n",
    "Annual pass journeys heavily focus around one particular journey duration, whereas the other passes are spread much wider. \n",
    "The other pass types seem to be distributed in a similar manner as they have roughly the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bike_df_cleaned[['passholder_type', 'duration']].boxplot(by='passholder_type', ax=ax, showfliers=False)\n",
    "ax.set_yscale('log')\n",
    "plt.ylabel('duration')\n",
    "plt.xlabel('passholder type');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the boxplots above with a log(duration) scale, it can be seen that Annual Pass users fit within a smaller range than the other passholder types and also have a lower interquartile range (IQR). This may be because Annual Pass holders may be daily commuters, so will be completing the same journeys, however cannot be for certain as the number of samples within each category is not known from this plot. \n",
    "\n",
    "One Day Pass and Walk-up passes have the highest average duration and the highest IQRs, potentially reflecting that these are not often used by commuters/people making short journeys. One Day passes are slightly positively skewed, whereas walk-up passes do not seem to be highly skewed.\n",
    "\n",
    "Flex and Monthly passes have the lowest average journey duration, and do not seem to be skewed.\n",
    "\n",
    "Despite the differences in average journey duration between Flex, Monthly, One Day, and Walk-up passes, they each have the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned['passholder_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given the number of each pass type, it is likely that it cannot be inferred that annual pass holders are distributed in a different fashion to the other distributions due to the number of samples within this category. This would need to be proven using a statistical test, such as A/B testing using a Chi-Squared test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Day Pass vs Flex Pass Mean Duration Statistical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the boxplots above, it seems there is a meaningful difference in the mean duration between these two passholder types, however it is still necessary to conduct a statistical test to be sure.\n",
    "\n",
    "To measure the similarity of the means of two distributions the Student's t-test is used. However this test assumes that the variance of both distributions is equal and also works better when the distributions contain a similar number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_pass_df = bike_df_cleaned[bike_df_cleaned['passholder_type'] == 'One Day Pass']\n",
    "flex_pass_df = bike_df_cleaned[bike_df_cleaned['passholder_type'] == 'Flex Pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_one_day_pass = np.var(one_day_pass_df['duration'].values)\n",
    "var_flex_pass = np.var(flex_pass_df['duration'].values)\n",
    "sample_size_difference = np.abs(len(flex_pass_df) - len(one_day_pass_df))\n",
    "var_difference = np.abs(var_one_day_pass - var_flex_pass)\n",
    "print(\"Sample size difference: {}\\tVariance difference: {}\".format(sample_size_difference, var_difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the difference in sample size and variance seems non-negligible; potentially another measure should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_pass_df['duration'].plot.hist(bins=100, logy=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_pass_df['duration'].plot.hist(bins=100, logy=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms for these distributions also seems to show a difference in both variance and number of samples\n",
    "\n",
    "A variant of the Student's t-test for equal means is Welch's t-test, which is more suited to the comparison of two distributions where they have differing variances and sample sizes.\n",
    "\n",
    "For this test:\n",
    "\n",
    "H0 -> Both distributions have equal means\n",
    "\n",
    "H1 -> The distributions have different means\n",
    "\n",
    "Assumptions:\n",
    "- The independant variable is categorical\n",
    "- Distribution of each should follow the normal distribution (However, using sample means with large sample sizes, Central Limit Theorum allows us to generate distributions from each that are approximately normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 125\n",
    "samples_one_day_pass = np.array_split(one_day_pass_df['duration'].sample(frac=1, random_state=0).values, int(len(one_day_pass_df)/sample_size))\n",
    "sample_means_one_day_pass = list(map(lambda x: np.mean(x), samples_one_day_pass))\n",
    "samples_flex_pass = np.array_split(flex_pass_df['duration'].sample(frac=1, random_state=0).values, int(len(flex_pass_df)/sample_size))\n",
    "sample_means_flex_pass = list(map(lambda x: np.mean(x), samples_flex_pass))\n",
    "\n",
    "sample_means_one_day_pass_var = np.var(sample_means_one_day_pass)\n",
    "sample_means_flex_pass_var = np.var(sample_means_flex_pass)\n",
    "\n",
    "plt.hist(sample_means_one_day_pass)\n",
    "plt.title(\"One Day pass\")\n",
    "plt.show()\n",
    "print(\"Variance: {}\".format(sample_means_one_day_pass_var))\n",
    "\n",
    "plt.title(\"Flex pass\")\n",
    "plt.hist(sample_means_flex_pass)\n",
    "plt.show()\n",
    "print(\"Variance: {}\".format(sample_means_flex_pass_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Student's t-test:\")\n",
    "\n",
    "t_test = ttest_ind(one_day_pass_df['duration'], flex_pass_df['duration'], equal_var=True)\n",
    "t_test2 = ttest_ind(sample_means_one_day_pass, sample_means_flex_pass, equal_var=True)\n",
    "\n",
    "print(\"\\tOriginal t-test p-value: {}\\tSample means t-test p-value: {}\".format(t_test.pvalue, t_test2.pvalue))\n",
    "\n",
    "print(\"\\nWelch's t-test:\")\n",
    "\n",
    "t_test = ttest_ind(one_day_pass_df['duration'], flex_pass_df['duration'], equal_var=False)\n",
    "t_test2 = ttest_ind(sample_means_one_day_pass, sample_means_flex_pass, equal_var=False)\n",
    "\n",
    "print(\"\\tOriginal t-test p-value: {}\\tSample means t-test p-value: {}\".format(t_test.pvalue, t_test2.pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two p-values from the Welch t-tests conducted it can be seen that using the sample means rather than original values does have an effect on the p-value score as the original distributions were not normally distributed, whereas the sample means distributions approximate a normal distribution.\n",
    "\n",
    "The impact of performing a Welch's t-test vs a Student's t-test is also shown by the difference in results between the two; despite this, in this example all tests provide the same outcome; allowing for the rejection of H0, and acceptance of H1. Implying that the means of the two samples are unequal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert columns to datetime\n",
    "As can be seen below, the start_time and end_time columns are not 'datetime' format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None # stops SettingWithCopy Warning from being displayed where not required\n",
    "bike_df_cleaned['start_time'] = pd.to_datetime(bike_df_cleaned['start_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "bike_df_cleaned['end_time'] = pd.to_datetime(bike_df_cleaned['end_time'], format='%Y-%m-%d %H:%M:%S') \n",
    "bike_df_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New column for hour of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_cleaned['hour_of_day'] = bike_df_cleaned.apply(lambda x: x['start_time'].hour, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of the effect of start hour on the journey duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bike_df_cleaned[['hour_of_day', 'duration']].boxplot(by='hour_of_day', ax=ax, showfliers=False)\n",
    "ax.set_yscale('log')\n",
    "ax.axhline(bike_df_cleaned['duration'].median())\n",
    "plt.ylabel('Duration')\n",
    "plt.xlabel('Hour of day');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = bike_df_cleaned[['hour_of_day', 'duration']].groupby('hour_of_day').apply(lambda x: np.var(x['duration']))\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of samples at each hour\")\n",
    "print(bike_df_cleaned[['hour_of_day', 'duration']].groupby('hour_of_day').apply(lambda x: len(x['duration'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplots above seem to indicate that between 3 AM and 10 AM the average journey duration is lower than the median journey duration. The variance of the journeys between 5 AM and 8 AM is also lower than the other times, potentially indicating that most of these journeys are similar and is likely that these represent the commuter traffic.\n",
    "\n",
    "The range of the values for times 3 AM until 5 AM was also lower than that of the other times, however this may be partly due to a lower number of samples within these time ranges as shown above. Despite this, 2 AM has a much wider range, despite also having a relatively low number of samples, potentially indicating a more meaningful difference in distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_3am_10am = bike_df_cleaned[(bike_df_cleaned['hour_of_day'] <=10) & (bike_df_cleaned['hour_of_day'] >=3)]\n",
    "bike_df_other_times = bike_df_cleaned[(bike_df_cleaned['hour_of_day'] >=10) | (bike_df_cleaned['hour_of_day'] <=3)]\n",
    "sample_size = 125\n",
    "\n",
    "samples_bike_df_3am_10am = np.array_split(bike_df_3am_10am['duration'].sample(frac=1, random_state=0).values, int(len(bike_df_3am_10am)/sample_size))\n",
    "sample_means_bike_df_3am_10am = list(map(lambda x: np.mean(x), samples_bike_df_3am_10am))\n",
    "samples_bike_df_other_times = np.array_split(bike_df_other_times['duration'].sample(frac=1, random_state=0).values, int(len(bike_df_other_times)/sample_size))\n",
    "sample_means_bike_df_other_times = list(map(lambda x: np.mean(x), samples_bike_df_other_times))\n",
    "\n",
    "print(\"t-test p-value: {}\".format(ttest_ind(sample_means_bike_df_3am_10am, sample_means_bike_df_other_times, equal_var=False).pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mann-Whitney U-test p-value: {}\".format(mannwhitneyu(bike_df_3am_10am['duration'], bike_df_other_times['duration']).pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to previous t-tests, as the p-value is so low (p < 0.05) it is statistically unlikely that the means for both distributions are equal.\n",
    "\n",
    "The Mann-Whitney U-test p-value of ~0.0 also indicates that there is a statistically significant difference between these distributions, as p < 0.05. The distributions may differ because of the types of journeys being made between 3 am and 10 am; likely being commuters using the bikes to get to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of the effect of start day on the journey duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "week_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "bike_df_cleaned['day_of_week'] = bike_df_cleaned.apply(lambda x: x['start_time'].weekday(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bike_df_cleaned[['day_of_week', 'duration']].boxplot(by='day_of_week', ax=ax, labels=week_days, showfliers=False)\n",
    "ax.set_yscale('log')\n",
    "ax.axhline(bike_df_cleaned['duration'].median())\n",
    "plt.ylabel('Duration')\n",
    "plt.xlabel('Day of week');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This boxplot seems to indicate that there is a difference in trip length between week days and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_weekday = bike_df_cleaned[bike_df_cleaned['day_of_week'] <=4]\n",
    "bike_df_weekend = bike_df_cleaned[bike_df_cleaned['day_of_week'] >=5]\n",
    "\n",
    "sample_size = 350\n",
    "samples_bike_df_weekday = np.array_split(bike_df_weekday['duration'].sample(frac=1, random_state=0).values, int(len(bike_df_weekday)/sample_size))\n",
    "sample_means_bike_df_weekday = list(map(lambda x: np.mean(x), samples_bike_df_weekday))\n",
    "plt.hist(sample_means_bike_df_weekday)\n",
    "plt.title(\"Sample means for weekday journeys\")\n",
    "plt.show()\n",
    "\n",
    "samples_bike_df_weekend = np.array_split(bike_df_weekend['duration'].sample(frac=1, random_state=0).values, int(len(bike_df_weekend)/sample_size))\n",
    "sample_means_bike_df_weekend = list(map(lambda x: np.mean(x), samples_bike_df_weekend))\n",
    "plt.hist(sample_means_bike_df_weekend)\n",
    "plt.title(\"Sample means for weekend journeys\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sample_means_bike_df_weekday_var = np.var(sample_means_bike_df_weekday)\n",
    "sample_means_bike_df_weekend_var = np.var(sample_means_bike_df_weekend)\n",
    "t_test = ttest_ind(sample_means_bike_df_weekend, sample_means_bike_df_weekday, equal_var=False)\n",
    "print(\"Weekday mean: {}\\t Weekend mean:{}\".format(np.mean(bike_df_weekday['duration']),np.mean(bike_df_weekend['duration'])))\n",
    "print(\"Weekday var: {}\\tWeekend var: {}\".format(sample_means_bike_df_weekday_var, sample_means_bike_df_weekend_var))\n",
    "print(\"Weekend-weekday t-test p-value: {}\".format(t_test.pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sample means to generate normal distributions from the approximately beta distributed duration field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_weekday['duration'].plot.hist(log=True, bins=100);\n",
    "plt.title(\"Weekday journey duration histrogram\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_weekend['duration'].plot.hist(log=True, bins=100);\n",
    "plt.title(\"Weekend journey duration histrogram\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mann-Whitney U-test p-value: {}\".format(mannwhitneyu(bike_df_weekday['duration'], bike_df_weekend['duration']).pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the histograms above do seem to have the same shape, the t-test p-value shows that they are extremely unlikely to have equal means. The Mann-Whitney U-test p-value of ~0.0 suggests that is extremely unlikely for the weekend and weekday durations to come from the same distribution as p < 0.05 (rejecting H0 of both coming from the same distribution). This dissimilarity is also highlighted when comparing the mean and variance of the durations for weekend and weekday journeys; showing the weekend journeys to be longer on average, with a larger spread of values. These tests are in line with the predictions made earlier about weekend journeys having a longer duration on average than weekday journeys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of passholders on each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "passholders_per_day = bike_df_cleaned.groupby(['day_of_week', 'passholder_type'])['day_of_week'].count()\n",
    "fix, ax = plt.subplots()\n",
    "passholders_per_day.unstack().plot(logy=True, \n",
    "                                   title='Number of journeys per day for each passholder type',\n",
    "                                   ax=ax,\n",
    "                                   xticks=range(7)\n",
    "                                  )\n",
    "ax.set_xticklabels(week_days)\n",
    "ax.set_ylabel('Number of journeys')\n",
    "ax.set_xlabel('Day');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the line graph above it can be seen that monthly pass users mostly use their passes Monday to Friday, with a slight reduction in journeys for that passholder type through the weekend. It is likely that this is the type of pass used by most Monday to Friday commuters for their commutes to work; many still using their passes for journeys during the weekend.\n",
    "\n",
    "The annual pass seems to be an outlier here, as the number of journeys made using the annual pass is just 10 within our cleaned dataset (12 in the entire dataset). This makes it very hard to compare to other pass types on a day-by-day basis as the only days that have journeys for this pass are Saturday and Sunday. This pass is extremely uncommon, and potentially even has just one user.\n",
    "\n",
    "The walk-up pass is a very common pass both on weekdays and weekends; seeing a notable rise for the weekends. Due to these trends, this pass may be popular for both commuters and tourists within the city.\n",
    "\n",
    "The one-day pass is far less popular than the walk-up and monthly passes and has a slightly less stable distribution of journeys throughout the weekdays than the monthly and walk-up passes. This pass, similarly to the walk-up pass, has a slight rise in number of journeys made during the weekends; potentially being a popular mode of transport for tourists.\n",
    "\n",
    "The flex pass, similarly to the one-day pass, is far less popular than the walk-up and monthly passes. However, unlike the one-day pass, the flex pass is far more stable for weekdays and similarly to the monthly pass, it has a slight decline in number of journeys for weekends. This may indicate that this pass is mainly used by Monday-Friday commuters for work, but is still somewhat used during the weekends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network of stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weighted_edge(G, a, b):\n",
    "    if G.has_edge(a,b):\n",
    "        if isinstance(G, nx.MultiDiGraph):\n",
    "            G[a][b][0]['weight'] += 1\n",
    "        elif isinstance(G, nx.DiGraph) or isinstance(G, nx.Graph):\n",
    "            G[a][b]['weight'] += 1\n",
    "        else:\n",
    "            raise TypeError(\"Unexpected type\")\n",
    "    else:\n",
    "        G.add_edge(a, b, weight=1)\n",
    "                \n",
    "G = nx.DiGraph()\n",
    "bike_df_cleaned[['start_station', 'end_station']].apply(lambda x: add_weighted_edge(G, x['start_station'], x['end_station']), axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a weighted directed graph from start station, end station tuples. Incrementing edge weights for each "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network visualization\n",
    "**Note**: \n",
    "- Bokeh visualization of networkx graphs seems to be broken in Python 3.7, therefore the matplotlib visualization will be used instead.\n",
    "- Visualization of self loops is not supported with matplotlib visualization, so not all journeys are captured within the visualization.\n",
    "- If graphviz and pydot were installed code similar to the following could be used to visualize the graph, including these self-loops:\n",
    "\n",
    "    ```python\n",
    "    nx.write_dot(G,'graph.dot')    \n",
    "    !neato -T png graph.dot > graph.png\n",
    "    graph_image = plt.imread(\"graph.png\")\n",
    "    plt.imshow(graph_image, cmap='gray')\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "draw_edge_labels = False\n",
    "\n",
    "def generate_node_colours(G, number_of_colours=None):\n",
    "    node_colours = nx.degree(G)\n",
    "    node_colours = np.array(list(map(lambda x: x[1],list(node_colours))))\n",
    "    # get ordered degree list (by station id) as np array for vectorization of functions\n",
    "    if number_of_colours is not None:\n",
    "        # node_colours is in range 0 .. number_of_colours-1\n",
    "        node_colours = (node_colours*(number_of_colours-1)/np.max(node_colours)).astype(int)\n",
    "    else:\n",
    "        node_colours = node_colours/np.max(node_colours)\n",
    "    return node_colours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `draw_edge_labels` switch can be changed to `True` if visualization of edge labels is desired. This clutters the graph and should only be used when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "# both calls to that matplotlib magic are required\n",
    "\n",
    "# kamada kawai force directed layout\n",
    "layout = nx.kamada_kawai_layout(G)\n",
    "if draw_edge_labels:\n",
    "    nx.draw_networkx_edge_labels(G, layout, edge_labels = nx.get_edge_attributes(G, 'weight'))\n",
    "nx.draw(G, pos=layout, node_color=generate_node_colours(G), with_labels=True, cmap=plt.cm.seismic)\n",
    "# use cmap seismic as it is 'divergent', contrasting well connected and less well connected nodes within the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph visualisation above it may be interpreted that there are two main groups of well connected stations (central nodes in red and white, and left nodes in blue) where within these groups there are many connections between the stations, and between these groups there are fewer connections. For this dataset this may represent short geographical distances within groups, and large geographical distances between groups.\n",
    "\n",
    "Outside of the two main groups, there are smaller groups of less well connected stations, with lower weighted degrees which is shown below using nodes 4214 and 4125 as examples well connected and less well connected nodes respectively. These stations may be geographically located in areas in which there are fewer journeys due to geographical distances between stations.\n",
    "\n",
    "Visualising the graph with edge labels (by setting the `draw_edge_labels` boolean to `True`) is nearly impossible to interpret. Despite this, it can be seen that nodes in blue have fewer low weighted edges between each other, whereas red nodes have more higher weighted edges between each other; therefore nodes in red have high weighted degrees, whereas nodes in blue has low weighted degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weighted degree for node {}: {}\".format(4125, G.degree(weight='weight')[4125]))\n",
    "print(\"Weighted degree for node {}: {}\".format(4214, G.degree(weight='weight')[4214]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the weighted degrees of a 'blue' node (4125) and a 'red' node (10811). This illustrates the difference in weighted degree between these 'blue' and 'red' nodes within the graph visualization above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_degree_histogram(G, n_bins = 25, weight=None):\n",
    "    # the networkx function to get degree histogram does not support weighted degrees\n",
    "    # this function serves the same purpose but allows for the specification of a weight variable\n",
    "    degrees = np.array(sorted(list(map(lambda x: x[1], list(G.degree(weight=weight))))))\n",
    "    #get degrees / weighted degrees of nodes\n",
    "    max_dist = max(degrees) / n_bins\n",
    "    #arbitrary number of bins selected by user\n",
    "    # range of each bin\n",
    "    degrees = np.round(np.round(degrees / max_dist) * max_dist).astype(int)\n",
    "    # group degrees by bins\n",
    "    degrees_counter_dict = Counter(degrees)\n",
    "    # count length of each bin\n",
    "    degrees = [degrees_counter_dict[i] if i in degrees_counter_dict else 0 for i in range(max(degrees))]\n",
    "    # insert missing degree values within 0 .. max degree as 0\n",
    "    # if not missing, insert count of that degree bin\n",
    "    return degrees[1:]\n",
    "\n",
    "weighted_degrees = calculate_degree_histogram(G, n_bins=250, weight='weight')\n",
    "degrees_histogram = calculate_degree_histogram(G, n_bins=250, weight=None)\n",
    "plt.loglog(range(len(weighted_degrees)), weighted_degrees, 'o');\n",
    "plt.ylabel(\"Frequency\");\n",
    "plt.xlabel(\"Weighted degree\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "plt.loglog(range(len(degrees_histogram)), degrees_histogram, 'o');\n",
    "plt.title(\"Degrees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the degree plots above it can be seen that the weighted degree distribution and degree distribution seem to follow a power law distribution (scale free graph) in a similar manner as to how is highlighted by Barabasi and Albert in the following paper https://arxiv.org/pdf/cond-mat/9910332.pdf%3Forigin%3Dpublication_detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_coeffs = nx.clustering(nx.Graph(G))\n",
    "plt.hist(list(clust_coeffs.values()), bins=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering coefficient histogram above shows that the egonets around each node are well connected, which is also shown by the network visualization above which shows that high connectivity within groups, and low connectivity between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = nx.betweenness_centrality(G)\n",
    "ec = nx.eigenvector_centrality(G)\n",
    "dc = nx.degree_centrality(G)\n",
    "deg = {x[0] : x[1] for x in nx.degree(G)}\n",
    "\n",
    "def get_top_n_centralities(df, n=5, centrality='betweenness', ascending=False):\n",
    "    return df.sort_values(centrality, ascending=ascending).head(n)\n",
    "\n",
    "centralities = pd.DataFrame()\n",
    "centralities['betweenness'] = list(map(lambda x: x[1] ,bc.items()))\n",
    "centralities.index = list(map(lambda x: x[0], bc.items()))\n",
    "centralities['eigenvalue'] = list(map(lambda x: x[1], ec.items()))\n",
    "centralities['degree_centrality'] = list(map(lambda x: x[1], dc.items()))\n",
    "centralities['node_degree'] = list(map(lambda x: x[1], deg.items()))\n",
    "print('betweenness:')\n",
    "print(get_top_n_centralities(centralities))\n",
    "print('\\neigenvalue:')\n",
    "print(get_top_n_centralities(centralities, 5, 'eigenvalue'))\n",
    "print('\\ndegree centrality:')\n",
    "print(get_top_n_centralities(centralities, 5, 'degree_centrality'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the top centrality scores as above shows that the highest ranked nodes are likely to be ranked high in all of these centrality scores. For example, station 3005 is placed within the top 5 stations for centrality within all methods of centrality scoring.\n",
    "\n",
    "Nodes with a high betweenness centrality score are commonly found within the shortest paths through the network and are therefore more likely to have a higher than average degree or a particularly important location for connecting multiple groups within a graph.\n",
    "\n",
    "Eigenvalue and degree centrality rely far more heavily on the node degree than betweenness centrality. Degree centrality is a centrality score determined by the degree of a node relative to the node with the highest degree within a network. Eigenvalue centrality is a score determined by the degree of a node and the nodes connected to it.\n",
    "\n",
    "The correlation matrix above confirms that eigenvalue and degree centrality are degree-based centrality scores, whereas betweenness centrality scores are based off of a different scoring metric, which in this case is the percentage of shortest paths a node is within."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assortativity coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_assortativity_coefficient(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assortativity coefficient above shows that a node is more likely to be connected to another node of a similar degree, therefore there are likely to be groups of well connected nodes, and groups of not so well connected nodes; where there are not so many connections between these nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to random graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(G.nodes())\n",
    "num_edges = len(G.edges())\n",
    "average_edges = num_edges / (num_nodes * (num_nodes-1)/2)\n",
    "avg_cc = []\n",
    "avg_asc = []\n",
    "iterations = 500\n",
    "for i in tqdm(range(iterations)):\n",
    "    g_test = nx.erdos_renyi_graph(num_nodes, average_edges)\n",
    "    avg_cc.append(nx.average_clustering(g_test))\n",
    "    avg_asc.append(nx.degree_assortativity_coefficient(g_test))\n",
    "avg_cc = sum(avg_cc) / len(avg_cc)\n",
    "avg_asc = sum(avg_asc) / len(avg_asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bike station clustering coeff:\\t{}\\nEerdos Renyi clustering coeff:\\t{}\".format(nx.average_clustering(nx.Graph(G)), avg_cc))\n",
    "print(\"Bike station assortativity:\\t{}\\nErdos Renyi assortativity:\\t{}\".format(nx.degree_assortativity_coefficient(G), avg_asc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a reasonable difference in clustering coefficients between the bike station and randomly generated graphs, which may be a further reason to believe that the graph of stations is not randomly generated, and in fact may be due to a different process of generation, such as a scale free graph. This is also shown the the large difference in assortativity between the two graphs too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to scale-free graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cc = []\n",
    "avg_asc = []\n",
    "iterations = 500\n",
    "average_edges = round(num_edges / num_nodes)\n",
    "\n",
    "for i in tqdm(range(iterations)):\n",
    "    g_test = nx.generators.barabasi_albert_graph(num_nodes, average_edges)\n",
    "    avg_cc.append(nx.average_clustering(g_test))\n",
    "    avg_asc.append(nx.degree_assortativity_coefficient(g_test))\n",
    "avg_cc = sum(avg_cc) / len(avg_cc)\n",
    "avg_asc = sum(avg_asc) / len(avg_asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bike station clustering coeff:\\t\\t\\t{}\\nBarabasi-Albert Scale-free clustering coeff:\\t{}\".format(nx.average_clustering(nx.Graph(G)), avg_cc))\n",
    "print(\"Bike station assortativity:\\t\\t\\t{}\\nBarabasi-Albert Scale-free assortativity:\\t{}\".format(nx.degree_assortativity_coefficient(G), avg_asc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bike station graph is also not well represented by graphs created using the Barabasi-Albert model for preferential attachment, therefore the graph structure within the bike station is neither random, not formed via a perferential attachment model. A more complex model taking into account the geospatial locality of each station to landmarks and often travelled routes within the city would likely explain this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Seeds dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_df = pd.read_csv(\"data/seeds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the fields contain null values, so no imputing values or removal or rows / columns is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_df.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 210 different seed samples.\n",
    "\n",
    "The correlation matrix above shows that many of the variables within the dataset are highly correlated, for example area and perimeter with a pearson correlation coefficient of 0.994341. This similarity between area and perimeter is also shown by the very high covariance within the variance-covariance matrix.\n",
    "\n",
    "It is likely that dimensionality reduction techniques, such as Principal Component Analysis (PCA) may increase the effectiveness of euclidean-distance based techniques due to the 'curse of dimensionality' associated with such distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.pairplot(seeds_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairs plots above also show large numbers of strong positive correlations between the variables.\n",
    "\n",
    "The histograms of each variable are also able to indicate features within each variable, for example groove length is potentially a bimodal distribution and may allow for two distinct clusters to be determined.\n",
    "\n",
    "Width seems to be nearly uniformally distributed, however further investigation would be required to determine whether this claim is statistically significant.\n",
    "\n",
    "Asymmetry seems to follow a positively skewed negative binomial or poisson distribution, whereas compactness seems to have negative skew. \n",
    "\n",
    "Perimeter, and length seems to be fairly normally distributed.\n",
    "\n",
    "Area may be a positively skewed binomial or poisson distribution, however this is not very clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "For Principal Component Analysis (PCA) of this dataset a cumulative variance threshold of 0.8 shall be used. Meaning that between all principal components 80% or more of the variance within the dataset will be explained by those principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_pca_ = PCA(n_components=0.8)\n",
    "seeds_df_pca = seeds_pca_.fit_transform(StandardScaler().fit_transform(seeds_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data before PCA creates a more balanced principal component model; preventing one variable with a high variance from 'dominating' other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of principal components: {}\".format(seeds_pca_.n_components_))\n",
    "print(\"Variance of the original dataset explained by each principal component:\")\n",
    "for i, pc in enumerate(seeds_pca_.explained_variance_ratio_):\n",
    "    print(\"\\tPrincial component {}: {:.2f}%\".format(i, pc*100))\n",
    "print(\"Cumulative sum of variance of the original dataset explained by all principal components: {:.2f}%\".format(sum(seeds_pca_.explained_variance_ratio_*100)))\n",
    "print(\"This dimensionality reduction has effectively reduced the dimensionality of the dataset to {:.2f}% of the original\".format(seeds_pca_.n_components_*100/len(seeds_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Gaussian Mixtures to the data\n",
    "Separate gaussian mixtures will be used to compare the effects of using the PCA data or using the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(2, random_state=0)\n",
    "gm_pca = GaussianMixture(2, random_state=0)\n",
    "gm.fit(seeds_df)\n",
    "gm_pca.fit(seeds_df_pca);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
